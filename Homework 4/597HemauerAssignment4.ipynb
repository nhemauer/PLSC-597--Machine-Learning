{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfcf7675-2357-4092-a6e2-76f0e7074dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8b8c2de-69af-422f-b180-86eedb2b3623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x15044416b0f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)  # Seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5161eb1c-2d96-4764-8a05-17ff28105b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset in characters:  5427\n",
      "I am happy to join with you today in what will go down in history as the greatest demonstration for\n",
      "freedom in the history of our nation.\n",
      "Five score years ago a great American in whose symbolic shadow we stand today signed the\n",
      "Emancipation Proclamation. This momentous decree is a great beacon light of hope to millions of Negro\n",
      "slaves who had been seared in the flames of withering injustice. It came as a joyous daybreak to end the\n",
      "long night of their captivity. But 100 years later the Negro still is not free. One hundred years later the\n",
      "life of the Negro is still badly crippled by the manacles of segregation and the chains of discrimination.\n",
      "One hundred years later the Negro lives on a lonely island of poverty in the midst of a vast ocean of\n",
      "material prosperity. One hundred years later the Negro is still languished in the corners of American\n",
      "society and finds himself in exile in his own land. So we’ve come here today to dramatize a shameful\n",
      "condition.\n",
      "In a sense we’ve come to our nation\n"
     ]
    }
   ],
   "source": [
    "# Load and process the text data\n",
    "with open('input2.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"Length of dataset in characters: \", len(text))\n",
    "\n",
    "# Let's look at the first 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6ccef4d-8157-48b0-bf8f-42f950ca30e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Characters: \n",
      " ,-.0124:?ABCDEFGHIJLMNOPRSTWYabcdefghijklmnopqrstuvwxyz©—’“”\n",
      "Vocabulary Size: 62\n",
      "Encoded: [38, 39, 39, 1, 50, 38, 35, 48, 35]\n",
      "Decoded: hii there\n"
     ]
    }
   ],
   "source": [
    "# Extracting and organizing the unique characters from the text\n",
    "chars = sorted(list(set(text)))  # Extracts unique characters, converts to a list, and sorts it\n",
    "vocab_size = len(chars)  # Counts the number of unique characters to determine vocabulary size\n",
    "\n",
    "# Using f-strings for better output formatting\n",
    "print('Unique Characters:', ''.join(chars))  # Prints all unique characters as a single string\n",
    "print(f'Vocabulary Size: {vocab_size}')  # Prints the vocabulary size we will be using\n",
    "\n",
    "# Creating mappings between characters and integers\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}  # Mapping from characters to integers\n",
    "itos = {i: ch for i, ch in enumerate(chars)}  # Mapping from integers to characters\n",
    "\n",
    "# Defining encoding and decoding functions\n",
    "encode = lambda s: [stoi[c] for c in s]  # Encodes a string into a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l])  # Decodes a list of integers back into a string\n",
    "\n",
    "# Testing the encoding and decoding functions\n",
    "encoded_str = encode(\"hii there\")  # Encoding a sample string\n",
    "print(f'Encoded: {encoded_str}')  # Printing the encoded representation\n",
    "\n",
    "decoded_str = decode(encoded_str)  # Decoding the encoded string\n",
    "print(f'Decoded: {decoded_str}')  # Printing the decoded string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92c0242b-be8c-4bde-8577-2c8a3a72ff1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5427]) torch.int64\n",
      "tensor([19,  1, 31, 43,  1, 38, 31, 46, 46, 55,  1, 50, 45,  1, 40, 45, 39, 44,\n",
      "         1, 53, 39, 50, 38,  1, 55, 45, 51,  1, 50, 45, 34, 31, 55,  1, 39, 44,\n",
      "         1, 53, 38, 31, 50,  1, 53, 39, 42, 42,  1, 37, 45,  1, 34, 45, 53, 44,\n",
      "         1, 39, 44,  1, 38, 39, 49, 50, 45, 48, 55,  1, 31, 49,  1, 50, 38, 35,\n",
      "         1, 37, 48, 35, 31, 50, 35, 49, 50,  1, 34, 35, 43, 45, 44, 49, 50, 48,\n",
      "        31, 50, 39, 45, 44,  1, 36, 45, 48,  0, 36, 48, 35, 35, 34, 45, 43,  1,\n",
      "        39, 44,  1, 50, 38, 35,  1, 38, 39, 49, 50, 45, 48, 55,  1, 45, 36,  1,\n",
      "        45, 51, 48,  1, 44, 31, 50, 39, 45, 44,  4,  0, 16, 39, 52, 35,  1, 49,\n",
      "        33, 45, 48, 35,  1, 55, 35, 31, 48, 49,  1, 31, 37, 45,  1, 31,  1, 37,\n",
      "        48, 35, 31, 50,  1, 11, 43, 35, 48, 39, 33, 31, 44,  1, 39, 44,  1, 53,\n",
      "        38, 45, 49, 35,  1, 49, 55, 43, 32, 45, 42, 39, 33,  1, 49, 38, 31, 34,\n",
      "        45, 53,  1, 53, 35,  1, 49, 50, 31, 44, 34,  1, 50, 45, 34, 31, 55,  1,\n",
      "        49, 39, 37, 44, 35, 34,  1, 50, 38, 35,  0, 15, 43, 31, 44, 33, 39, 46,\n",
      "        31, 50, 39, 45, 44,  1, 25, 48, 45, 33, 42, 31, 43, 31, 50, 39, 45, 44,\n",
      "         4,  1, 28, 38, 39, 49,  1, 43, 45, 43, 35, 44, 50, 45, 51, 49,  1, 34,\n",
      "        35, 33, 48, 35, 35,  1, 39, 49,  1, 31,  1, 37, 48, 35, 31, 50,  1, 32,\n",
      "        35, 31, 33, 45, 44,  1, 42, 39, 37, 38, 50,  1, 45, 36,  1, 38, 45, 46,\n",
      "        35,  1, 50, 45,  1, 43, 39, 42, 42, 39, 45, 44, 49,  1, 45, 36,  1, 23,\n",
      "        35, 37, 48, 45,  0, 49, 42, 31, 52, 35, 49,  1, 53, 38, 45,  1, 38, 31,\n",
      "        34,  1, 32, 35, 35, 44,  1, 49, 35, 31, 48, 35, 34,  1, 39, 44,  1, 50,\n",
      "        38, 35,  1, 36, 42, 31, 43, 35, 49,  1, 45, 36,  1, 53, 39, 50, 38, 35,\n",
      "        48, 39, 44, 37,  1, 39, 44, 40, 51, 49, 50, 39, 33, 35,  4,  1, 19, 50,\n",
      "         1, 33, 31, 43, 35,  1, 31, 49,  1, 31,  1, 40, 45, 55, 45, 51, 49,  1,\n",
      "        34, 31, 55, 32, 48, 35, 31, 41,  1, 50, 45,  1, 35, 44, 34,  1, 50, 38,\n",
      "        35,  0, 42, 45, 44, 37,  1, 44, 39, 37, 38, 50,  1, 45, 36,  1, 50, 38,\n",
      "        35, 39, 48,  1, 33, 31, 46, 50, 39, 52, 39, 50, 55,  4,  1, 12, 51, 50,\n",
      "         1,  6,  5,  5,  1, 55, 35, 31, 48, 49,  1, 42, 31, 50, 35, 48,  1, 50,\n",
      "        38, 35,  1, 23, 35, 37, 48, 45,  1, 49, 50, 39, 42, 42,  1, 39, 49,  1,\n",
      "        44, 45, 50,  1, 36, 48, 35, 35,  4,  1, 24, 44, 35,  1, 38, 51, 44, 34,\n",
      "        48, 35, 34,  1, 55, 35, 31, 48, 49,  1, 42, 31, 50, 35, 48,  1, 50, 38,\n",
      "        35,  0, 42, 39, 36, 35,  1, 45, 36,  1, 50, 38, 35,  1, 23, 35, 37, 48,\n",
      "        45,  1, 39, 49,  1, 49, 50, 39, 42, 42,  1, 32, 31, 34, 42, 55,  1, 33,\n",
      "        48, 39, 46, 46, 42, 35, 34,  1, 32, 55,  1, 50, 38, 35,  1, 43, 31, 44,\n",
      "        31, 33, 42, 35, 49,  1, 45, 36,  1, 49, 35, 37, 48, 35, 37, 31, 50, 39,\n",
      "        45, 44,  1, 31, 44, 34,  1, 50, 38, 35,  1, 33, 38, 31, 39, 44, 49,  1,\n",
      "        45, 36,  1, 34, 39, 49, 33, 48, 39, 43, 39, 44, 31, 50, 39, 45, 44,  4,\n",
      "         0, 24, 44, 35,  1, 38, 51, 44, 34, 48, 35, 34,  1, 55, 35, 31, 48, 49,\n",
      "         1, 42, 31, 50, 35, 48,  1, 50, 38, 35,  1, 23, 35, 37, 48, 45,  1, 42,\n",
      "        39, 52, 35, 49,  1, 45, 44,  1, 31,  1, 42, 45, 44, 35, 42, 55,  1, 39,\n",
      "        49, 42, 31, 44, 34,  1, 45, 36,  1, 46, 45, 52, 35, 48, 50, 55,  1, 39,\n",
      "        44,  1, 50, 38, 35,  1, 43, 39, 34, 49, 50,  1, 45, 36,  1, 31,  1, 52,\n",
      "        31, 49, 50,  1, 45, 33, 35, 31, 44,  1, 45, 36,  0, 43, 31, 50, 35, 48,\n",
      "        39, 31, 42,  1, 46, 48, 45, 49, 46, 35, 48, 39, 50, 55,  4,  1, 24, 44,\n",
      "        35,  1, 38, 51, 44, 34, 48, 35, 34,  1, 55, 35, 31, 48, 49,  1, 42, 31,\n",
      "        50, 35, 48,  1, 50, 38, 35,  1, 23, 35, 37, 48, 45,  1, 39, 49,  1, 49,\n",
      "        50, 39, 42, 42,  1, 42, 31, 44, 37, 51, 39, 49, 38, 35, 34,  1, 39, 44,\n",
      "         1, 50, 38, 35,  1, 33, 45, 48, 44, 35, 48, 49,  1, 45, 36,  1, 11, 43,\n",
      "        35, 48, 39, 33, 31, 44,  0, 49, 45, 33, 39, 35, 50, 55,  1, 31, 44, 34,\n",
      "         1, 36, 39, 44, 34, 49,  1, 38, 39, 43, 49, 35, 42, 36,  1, 39, 44,  1,\n",
      "        35, 54, 39, 42, 35,  1, 39, 44,  1, 38, 39, 49,  1, 45, 53, 44,  1, 42,\n",
      "        31, 44, 34,  4,  1, 27, 45,  1, 53, 35, 59, 52, 35,  1, 33, 45, 43, 35,\n",
      "         1, 38, 35, 48, 35,  1, 50, 45, 34, 31, 55,  1, 50, 45,  1, 34, 48, 31,\n",
      "        43, 31, 50, 39, 56, 35,  1, 31,  1, 49, 38, 31, 43, 35, 36, 51, 42,  0,\n",
      "        33, 45, 44, 34, 39, 50, 39, 45, 44,  4,  0, 19, 44,  1, 31,  1, 49, 35,\n",
      "        44, 49, 35,  1, 53, 35, 59, 52, 35,  1, 33, 45, 43, 35,  1, 50, 45,  1,\n",
      "        45, 51, 48,  1, 44, 31, 50, 39, 45, 44])\n"
     ]
    }
   ],
   "source": [
    "# Convert the entire text into a tensor of encoded integers\n",
    "data = torch.tensor(encode(text), dtype=torch.long)  # Encodes the text and converts it to a PyTorch tensor\n",
    "\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) # Encoding of the the 1000 characters printed above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "339bdf05-9ca7-4681-882f-ac18b9072f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([19]) the target: 1\n",
      "when input is tensor([19,  1]) the target: 31\n",
      "when input is tensor([19,  1, 31]) the target: 43\n",
      "when input is tensor([19,  1, 31, 43]) the target: 1\n",
      "when input is tensor([19,  1, 31, 43,  1]) the target: 38\n",
      "when input is tensor([19,  1, 31, 43,  1, 38]) the target: 31\n",
      "when input is tensor([19,  1, 31, 43,  1, 38, 31]) the target: 46\n",
      "when input is tensor([19,  1, 31, 43,  1, 38, 31, 46]) the target: 46\n"
     ]
    }
   ],
   "source": [
    "# Determine the split point for training and validation sets\n",
    "n = int(0.9 * len(data))  # Calculates 90% of the data length to use as the training set size\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data = data[:n]  # Assigns the first 90% of the data to training\n",
    "val_data = data[n:]  # Assigns the remaining 10% to validation\n",
    "\n",
    "block_size = 8\n",
    "train_data[:block_size+1] #block elements + 1 character sampled from the data\n",
    "\n",
    "x = train_data[:block_size]\n",
    "\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1] #varying contect\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0beb19c9-9b49-48cb-8fec-dc242f510ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[38, 35, 48, 35,  1, 50, 45, 34],\n",
      "        [34,  1, 50, 38, 35,  0, 15, 43],\n",
      "        [49,  1, 43, 51, 49, 50,  1, 32],\n",
      "        [45, 43, 43, 51, 44, 39, 50, 55]])\n",
      "----\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[35, 48, 35,  1, 50, 45, 34, 31],\n",
      "        [ 1, 50, 38, 35,  0, 15, 43, 31],\n",
      "        [ 1, 43, 51, 49, 50,  1, 32, 35],\n",
      "        [43, 43, 51, 44, 39, 50, 55,  1]])\n",
      "----\n",
      "when input is [38] the target: 35\n",
      "when input is [38, 35] the target: 48\n",
      "when input is [38, 35, 48] the target: 35\n",
      "when input is [38, 35, 48, 35] the target: 1\n",
      "when input is [38, 35, 48, 35, 1] the target: 50\n",
      "when input is [38, 35, 48, 35, 1, 50] the target: 45\n",
      "when input is [38, 35, 48, 35, 1, 50, 45] the target: 34\n",
      "when input is [38, 35, 48, 35, 1, 50, 45, 34] the target: 31\n",
      "when input is [34] the target: 1\n",
      "when input is [34, 1] the target: 50\n",
      "when input is [34, 1, 50] the target: 38\n",
      "when input is [34, 1, 50, 38] the target: 35\n",
      "when input is [34, 1, 50, 38, 35] the target: 0\n",
      "when input is [34, 1, 50, 38, 35, 0] the target: 15\n",
      "when input is [34, 1, 50, 38, 35, 0, 15] the target: 43\n",
      "when input is [34, 1, 50, 38, 35, 0, 15, 43] the target: 31\n",
      "when input is [49] the target: 1\n",
      "when input is [49, 1] the target: 43\n",
      "when input is [49, 1, 43] the target: 51\n",
      "when input is [49, 1, 43, 51] the target: 49\n",
      "when input is [49, 1, 43, 51, 49] the target: 50\n",
      "when input is [49, 1, 43, 51, 49, 50] the target: 1\n",
      "when input is [49, 1, 43, 51, 49, 50, 1] the target: 32\n",
      "when input is [49, 1, 43, 51, 49, 50, 1, 32] the target: 35\n",
      "when input is [45] the target: 43\n",
      "when input is [45, 43] the target: 43\n",
      "when input is [45, 43, 43] the target: 51\n",
      "when input is [45, 43, 43, 51] the target: 44\n",
      "when input is [45, 43, 43, 51, 44] the target: 39\n",
      "when input is [45, 43, 43, 51, 44, 39] the target: 50\n",
      "when input is [45, 43, 43, 51, 44, 39, 50] the target: 55\n",
      "when input is [45, 43, 43, 51, 44, 39, 50, 55] the target: 1\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    \n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) #randomizing data selection\n",
    "    \n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape) #4x8 tensor\n",
    "print(xb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "print('targets:')\n",
    "print(yb.shape) #4x8 tensor\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb4de3ed-e3ed-443e-b667-0e7682f020ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters and environment setup\n",
    "batch_size = 16  # Number of sequences processed in parallel\n",
    "block_size = 32  # Maximum context length for predictions\n",
    "\n",
    "max_iters = 5000  # Number of training iterations\n",
    "\n",
    "eval_interval = 100  # Interval for evaluating the model\n",
    "learning_rate = 1e-3  # Learning rate for optimizer\n",
    "eval_iters = 200  # Number of iterations for evaluation\n",
    "n_embd = 64  # Size of each embedding vector\n",
    "n_head = 4  # Number of attention heads\n",
    "n_layer = 4  # Number of transformer layers\n",
    "dropout = 0.0  # Dropout rate\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # Use GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f996d17-5a24-4ec0-9451-e69e29218dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get a batch of data\n",
    "def get_batch(split):\n",
    "    # Choose the appropriate dataset based on the 'split' argument\n",
    "    data = train_data if split == 'train' else val_data  \n",
    "\n",
    "    # Randomly select starting indices for sequences in the batch\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "\n",
    "    # Create input sequences (x) using the selected indices\n",
    "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
    "\n",
    "    # Create target output sequences (y), which are offset by one character\n",
    "    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n",
    "\n",
    "    # Move the input and target tensors to the specified device (GPU or CPU)\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd03b415-5798-419a-9b9d-801550e9bb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()  # Disables gradient calculations for efficiency\n",
    "def estimate_loss():\n",
    "    out = {}  # Dictionary to store the average losses for training and validation\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    # Iterate over both training and validation datasets\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)  # Initialize an array to store individual losses\n",
    "\n",
    "        # Compute loss over a number of iterations\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)  # Get a batch of data\n",
    "            logits, loss = model(X, Y)  # Forward pass through the model to compute loss\n",
    "            losses[k] = loss.item()  # Store the loss for this iteration\n",
    "\n",
    "        out[split] = losses.mean()  # Calculate the average loss for this dataset\n",
    "\n",
    "    model.train()  # Set the model back to training mode\n",
    "    return out  # Return the average losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11a1702c-32cc-4064-af32-5281f630ff9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        # Linear layers to transform input into key, query, and value vectors\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "\n",
    "        # Lower triangular matrix for masking in attention calculation\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract batch size (B), sequence length (T), and embedding dimension (C)\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # Compute key, query, and value vectors\n",
    "        k = self.key(x)   # Key vector\n",
    "        q = self.query(x) # Query vector\n",
    "\n",
    "        # Calculate attention scores\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5  # Scaled dot-product attention\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # Masking to prevent attending to future positions\n",
    "        wei = F.softmax(wei, dim=-1)  # Apply softmax to get attention weights\n",
    "        wei = self.dropout(wei)  # Apply dropout\n",
    "\n",
    "        # Compute weighted sum of values\n",
    "        v = self.value(x)  # Value vector\n",
    "        out = wei @ v  # Weighted sum of values\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89571d5a-5e21-475f-a2d8-ad7e2a257de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        # Creating multiple attention heads\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        # Linear layer for projecting the concatenated output\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Concatenating outputs from all attention heads\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        # Projecting concatenated output to original embedding size and applying dropout\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2b8c21d-c46e-4ba7-b8f5-0d0064cbb575",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        # Defining the feedforward network\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),  # First linear layer, expands dimension\n",
    "            nn.ReLU(),                      # Non-linear activation function\n",
    "            nn.Linear(4 * n_embd, n_embd),  # Second linear layer, projects back to original dimension\n",
    "            nn.Dropout(dropout),            # Dropout for regularization\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Passes the input through the feedforward network\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06f9e9b0-a891-4867-ab8e-0edc80376aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head  # Determining the size of each attention head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)  # Multi-head self-attention layer\n",
    "        self.ffwd = FeedFoward(n_embd)                    # Feedforward network\n",
    "        self.ln1 = nn.LayerNorm(n_embd)                  # Layer normalization after self-attention\n",
    "        self.ln2 = nn.LayerNorm(n_embd)                  # Layer normalization after feedforward network\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply self-attention and add the result to the original input (residual connection)\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        # Apply the feedforward network and add the result to the above output (residual connection)\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5262dba6-fd5f-40a8-9478-4fc9a6fc2409",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    # Inherits from the PyTorch Module class\n",
    "    def __init__(self):\n",
    "        super().__init__()  # Initialize the superclass (nn.Module)\n",
    "        # Embedding layer to convert token indices to embeddings\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        # Embedding layer for positional encodings\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        # Sequential container of Transformer blocks\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        # Layer normalization applied after the Transformer blocks\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        # Linear layer to map from embedding dimension to vocabulary size\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # Extracting batch size (B) and sequence length (T) from input indices\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # Getting token embeddings for input indices\n",
    "        tok_emb = self.token_embedding_table(idx)  # Embedding lookup for tokens (B,T,C)\n",
    "        # Generating position embeddings for each position in the sequence\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # Positional encoding (T,C)\n",
    "        # Combining token and position embeddings\n",
    "        x = tok_emb + pos_emb  # Summing token and position embeddings (B,T,C)\n",
    "\n",
    "        # Passing the combined embeddings through the Transformer blocks\n",
    "        x = self.blocks(x)  # Processed by Transformer blocks (B,T,C)\n",
    "        # Applying layer normalization to the output of the Transformer blocks\n",
    "        x = self.ln_f(x)  # Normalized output (B,T,C)\n",
    "\n",
    "        # Projecting the output to vocabulary size to get logits for next token prediction\n",
    "        logits = self.lm_head(x)  # Output logits (B,T,vocab_size)\n",
    "\n",
    "        # Compute loss if target tokens are provided\n",
    "        if targets is not None:\n",
    "            # Reshaping for loss computation\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            # Calculating cross-entropy loss\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        else:\n",
    "            loss = None  # No loss computation if targets are not provided\n",
    "\n",
    "        return logits, loss  # Returning logits and the loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # Generate text for a given number of tokens\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Keeping only the last 'block_size' tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # Get predictions for the current sequence\n",
    "            logits, _ = self(idx_cond)\n",
    "            # Focus on the last time step for prediction\n",
    "            logits = logits[:, -1, :]\n",
    "            # Convert logits to probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # Sample the next token from the probability distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # Append the sampled token to the sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx  # Return the generated sequence of tokens\n",
    "\n",
    "model = BigramLanguageModel()  # Instantiate the model\n",
    "m = model.to(device)  # Move the model to the specified device (CPU or GPU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a35b9b72-cab3-44de-af1c-a122e916a493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.209342 M parameters\n"
     ]
    }
   ],
   "source": [
    "# Summing up the number of elements (parameters) in each parameter tensor of the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f42618b-9643-44f1-a816-931f3a5d2781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an optimizer using the AdamW algorithm for the model's parameters\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "031b3f1d-3829-4178-bd56-0d5f71270d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.2804, val loss 4.3020\n",
      "step 100: train loss 2.5206, val loss 2.6041\n",
      "step 200: train loss 2.3421, val loss 2.4581\n",
      "step 300: train loss 2.1823, val loss 2.3070\n",
      "step 400: train loss 1.9277, val loss 2.2070\n",
      "step 500: train loss 1.7422, val loss 2.1280\n",
      "step 600: train loss 1.5069, val loss 2.0817\n",
      "step 700: train loss 1.3336, val loss 2.1217\n",
      "step 800: train loss 1.1177, val loss 2.1974\n",
      "step 900: train loss 0.9154, val loss 2.1888\n",
      "step 1000: train loss 0.7547, val loss 2.3044\n",
      "step 1100: train loss 0.6302, val loss 2.4746\n",
      "step 1200: train loss 0.5251, val loss 2.5164\n",
      "step 1300: train loss 0.4613, val loss 2.6419\n",
      "step 1400: train loss 0.4169, val loss 2.7466\n",
      "step 1500: train loss 0.3822, val loss 2.8236\n",
      "step 1600: train loss 0.3501, val loss 2.9377\n",
      "step 1700: train loss 0.3465, val loss 2.9975\n",
      "step 1800: train loss 0.3406, val loss 3.1250\n",
      "step 1900: train loss 0.3230, val loss 3.0770\n",
      "step 2000: train loss 0.3133, val loss 3.1518\n",
      "step 2100: train loss 0.3002, val loss 3.1376\n",
      "step 2200: train loss 0.2936, val loss 3.0364\n",
      "step 2300: train loss 0.2885, val loss 3.2643\n",
      "step 2400: train loss 0.2833, val loss 3.2427\n",
      "step 2500: train loss 0.2784, val loss 3.3521\n",
      "step 2600: train loss 0.2764, val loss 3.2908\n",
      "step 2700: train loss 0.2751, val loss 3.4476\n",
      "step 2800: train loss 0.2711, val loss 3.3007\n",
      "step 2900: train loss 0.2686, val loss 3.4128\n",
      "step 3000: train loss 0.2671, val loss 3.4416\n",
      "step 3100: train loss 0.2656, val loss 3.3301\n",
      "step 3200: train loss 0.2618, val loss 3.3553\n",
      "step 3300: train loss 0.2613, val loss 3.3488\n",
      "step 3400: train loss 0.2549, val loss 3.3558\n",
      "step 3500: train loss 0.2512, val loss 3.5565\n",
      "step 3600: train loss 0.2487, val loss 3.4929\n",
      "step 3700: train loss 0.2500, val loss 3.6098\n",
      "step 3800: train loss 0.2487, val loss 3.4379\n",
      "step 3900: train loss 0.2494, val loss 3.5846\n",
      "step 4000: train loss 0.2487, val loss 3.5087\n",
      "step 4100: train loss 0.2473, val loss 3.5591\n",
      "step 4200: train loss 0.2494, val loss 3.6296\n",
      "step 4300: train loss 0.2424, val loss 3.5546\n",
      "step 4400: train loss 0.2409, val loss 3.5762\n",
      "step 4500: train loss 0.2509, val loss 3.6614\n",
      "step 4600: train loss 0.2412, val loss 3.6159\n",
      "step 4700: train loss 0.2385, val loss 3.4922\n",
      "step 4800: train loss 0.2364, val loss 3.6249\n",
      "step 4900: train loss 0.2453, val loss 3.6535\n",
      "step 4999: train loss 0.2457, val loss 3.6635\n"
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters):\n",
    "    # Looping through the training iterations\n",
    "\n",
    "    # Conditional to evaluate model performance periodically\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        # Estimate loss on training and validation datasets\n",
    "        losses = estimate_loss()\n",
    "        # Print the current step and the estimated losses\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # Sampling a batch of data for training\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Forward pass: compute predicted outputs by passing inputs to the model\n",
    "    logits, loss = model(xb, yb)\n",
    "    # Zeroing the gradients of all optimized tensors\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "    # Perform a single optimization step (parameter update)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "968feda4-bf0b-4438-b512-98b28c24f325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "join with you today in what will go down in history as the greatest demonstration for\n",
      "freedom ring from the snowners was lons of Neee’vel, come come tour nation’s capital to cash a check. When the architects of our Republic wrote\n",
      "the magnificent words votels of civil rights, “Whiten will you be\n",
      "satisfied?” We can nation sigull, bert  thate areatele ons of the highways and the hotels of the cities.\n",
      "We cannot be satisfied as long as the Negro’s basic mobility is crippline. Only This note was a promise that all men—yes, thatens  weltel wo ring injustice. It came as a joyous daybreak to end the\n",
      "long night of their captivity. But 100 years later the Negro is the videncotence. . . . The marvely come richteat of American\n",
      "society and finds himself in exile ily is tiew\n",
      "ll all men are created equalittle in exile ily is tied up\n",
      "with our destiny.\n",
      ". . . We cannot walk alone. And as we walk we must make the ple, cannot gur beack. Then are those who are. Let freedom ring from the snowcapped Rockies of Colorado. Let freedom ring from the curvaceous slopes of\n",
      "California.\n",
      "But not only that. Let freedom ring from the heire is a great stizelf in exile ily is tied up\n",
      "with our destiny.\n",
      ". . . We cannot walk alone. And as we walk we must make the ple, for ever condulther of their character. I have a dream that one\n",
      "day in Alabama, is the will bew boys and whitere promity. But 10101014 Thave dream dream that my four little children will one. We must not allow our\n",
      "creative protests to degenerate into physical viole. Let freedom ring from the snowcapped Rockies of Colorado. Let freedom ring from the heightening Alleghenies of Pennsylvania. Let\n",
      "freedom ring from the snowcapped Rockies of Colorado. Let freedom ring from the unspeakable horrors of\n",
      "police brutality.\n",
      "We can never be satisfied until justice rolls do. Let freedom ring from the mighty stream. . . .\n",
      "I signy this mobent tous decree is a great beacon light of hope to millions of Negro\n",
      "slaves who had been seared in the flames of will beri\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c84e7040-9236-42a1-945c-f3cf52e9bbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0ba5b45a-902c-415a-a1b2-3cb27c7ea659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters and environment setup\n",
    "batch_size = 32  # Number of sequences processed in parallel\n",
    "block_size = 64  # Maximum context length for predictions\n",
    "\n",
    "max_iters = 5000  # Number of training iterations\n",
    "\n",
    "eval_interval = 100  # Interval for evaluating the model\n",
    "learning_rate = 1e-3  # Learning rate for optimizer\n",
    "eval_iters = 200  # Number of iterations for evaluation\n",
    "n_embd = 64  # Size of each embedding vector\n",
    "n_head = 4  # Number of attention heads\n",
    "n_layer = 4  # Number of transformer layers\n",
    "dropout = 0.0  # Dropout rate\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # Use GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ec1d3e5c-db2f-4038-ba59-35972278b677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get a batch of data\n",
    "def get_batch(split):\n",
    "    # Choose the appropriate dataset based on the 'split' argument\n",
    "    data = train_data if split == 'train' else val_data  \n",
    "\n",
    "    # Randomly select starting indices for sequences in the batch\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "\n",
    "    # Create input sequences (x) using the selected indices\n",
    "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
    "\n",
    "    # Create target output sequences (y), which are offset by one character\n",
    "    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n",
    "\n",
    "    # Move the input and target tensors to the specified device (GPU or CPU)\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "432b3bb5-c544-43f6-a1b3-f906ed57ddae",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()  # Disables gradient calculations for efficiency\n",
    "def estimate_loss():\n",
    "    out = {}  # Dictionary to store the average losses for training and validation\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    # Iterate over both training and validation datasets\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)  # Initialize an array to store individual losses\n",
    "\n",
    "        # Compute loss over a number of iterations\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)  # Get a batch of data\n",
    "            logits, loss = model(X, Y)  # Forward pass through the model to compute loss\n",
    "            losses[k] = loss.item()  # Store the loss for this iteration\n",
    "\n",
    "        out[split] = losses.mean()  # Calculate the average loss for this dataset\n",
    "\n",
    "    model.train()  # Set the model back to training mode\n",
    "    return out  # Return the average losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "713a4f06-5173-4292-9cbe-2bb98706fa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        # Linear layers to transform input into key, query, and value vectors\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "\n",
    "        # Lower triangular matrix for masking in attention calculation\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract batch size (B), sequence length (T), and embedding dimension (C)\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # Compute key, query, and value vectors\n",
    "        k = self.key(x)   # Key vector\n",
    "        q = self.query(x) # Query vector\n",
    "\n",
    "        # Calculate attention scores\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5  # Scaled dot-product attention\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # Masking to prevent attending to future positions\n",
    "        wei = F.softmax(wei, dim=-1)  # Apply softmax to get attention weights\n",
    "        wei = self.dropout(wei)  # Apply dropout\n",
    "\n",
    "        # Compute weighted sum of values\n",
    "        v = self.value(x)  # Value vector\n",
    "        out = wei @ v  # Weighted sum of values\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3c009cd3-4c68-4624-9d90-e4b8c8b082b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        # Creating multiple attention heads\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        # Linear layer for projecting the concatenated output\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Concatenating outputs from all attention heads\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        # Projecting concatenated output to original embedding size and applying dropout\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "55378103-319f-4fc9-ae3d-85c83e4d56f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        # Defining the feedforward network\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),  # First linear layer, expands dimension\n",
    "            nn.ReLU(),                      # Non-linear activation function\n",
    "            nn.Linear(4 * n_embd, n_embd),  # Second linear layer, projects back to original dimension\n",
    "            nn.Dropout(dropout),            # Dropout for regularization\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Passes the input through the feedforward network\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1aba022f-5ecf-4011-a909-90feb08eb877",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head  # Determining the size of each attention head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)  # Multi-head self-attention layer\n",
    "        self.ffwd = FeedFoward(n_embd)                    # Feedforward network\n",
    "        self.ln1 = nn.LayerNorm(n_embd)                  # Layer normalization after self-attention\n",
    "        self.ln2 = nn.LayerNorm(n_embd)                  # Layer normalization after feedforward network\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply self-attention and add the result to the original input (residual connection)\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        # Apply the feedforward network and add the result to the above output (residual connection)\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "46650edd-b4d5-42ac-83ad-ed476ad03caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    # Inherits from the PyTorch Module class\n",
    "    def __init__(self):\n",
    "        super().__init__()  # Initialize the superclass (nn.Module)\n",
    "        # Embedding layer to convert token indices to embeddings\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        # Embedding layer for positional encodings\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        # Sequential container of Transformer blocks\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        # Layer normalization applied after the Transformer blocks\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        # Linear layer to map from embedding dimension to vocabulary size\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # Extracting batch size (B) and sequence length (T) from input indices\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # Getting token embeddings for input indices\n",
    "        tok_emb = self.token_embedding_table(idx)  # Embedding lookup for tokens (B,T,C)\n",
    "        # Generating position embeddings for each position in the sequence\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # Positional encoding (T,C)\n",
    "        # Combining token and position embeddings\n",
    "        x = tok_emb + pos_emb  # Summing token and position embeddings (B,T,C)\n",
    "\n",
    "        # Passing the combined embeddings through the Transformer blocks\n",
    "        x = self.blocks(x)  # Processed by Transformer blocks (B,T,C)\n",
    "        # Applying layer normalization to the output of the Transformer blocks\n",
    "        x = self.ln_f(x)  # Normalized output (B,T,C)\n",
    "\n",
    "        # Projecting the output to vocabulary size to get logits for next token prediction\n",
    "        logits = self.lm_head(x)  # Output logits (B,T,vocab_size)\n",
    "\n",
    "        # Compute loss if target tokens are provided\n",
    "        if targets is not None:\n",
    "            # Reshaping for loss computation\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            # Calculating cross-entropy loss\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        else:\n",
    "            loss = None  # No loss computation if targets are not provided\n",
    "\n",
    "        return logits, loss  # Returning logits and the loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # Generate text for a given number of tokens\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Keeping only the last 'block_size' tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # Get predictions for the current sequence\n",
    "            logits, _ = self(idx_cond)\n",
    "            # Focus on the last time step for prediction\n",
    "            logits = logits[:, -1, :]\n",
    "            # Convert logits to probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # Sample the next token from the probability distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # Append the sampled token to the sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx  # Return the generated sequence of tokens\n",
    "\n",
    "model = BigramLanguageModel()  # Instantiate the model\n",
    "m = model.to(device)  # Move the model to the specified device (CPU or GPU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b569fcbd-4ff9-4927-b0fe-e92fab4a8bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21139 M parameters\n"
     ]
    }
   ],
   "source": [
    "# Summing up the number of elements (parameters) in each parameter tensor of the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9a950f17-307f-459f-85ba-a0a38e269f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an optimizer using the AdamW algorithm for the model's parameters\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6a93584d-4609-4ee4-b6eb-c4638cd24b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.2224, val loss 4.2079\n",
      "step 100: train loss 2.4728, val loss 2.5442\n",
      "step 200: train loss 2.3185, val loss 2.4138\n",
      "step 300: train loss 2.1517, val loss 2.3698\n",
      "step 400: train loss 1.9202, val loss 2.2575\n",
      "step 500: train loss 1.6052, val loss 2.2271\n",
      "step 600: train loss 1.2250, val loss 2.2409\n",
      "step 700: train loss 0.8427, val loss 2.3793\n",
      "step 800: train loss 0.5431, val loss 2.5915\n",
      "step 900: train loss 0.3674, val loss 2.8513\n",
      "step 1000: train loss 0.2908, val loss 3.0762\n",
      "step 1100: train loss 0.2476, val loss 3.2044\n",
      "step 1200: train loss 0.2162, val loss 3.3692\n",
      "step 1300: train loss 0.2116, val loss 3.3895\n",
      "step 1400: train loss 0.1890, val loss 3.5368\n",
      "step 1500: train loss 0.1784, val loss 3.7319\n",
      "step 1600: train loss 0.1770, val loss 3.7772\n",
      "step 1700: train loss 0.1601, val loss 3.7662\n",
      "step 1800: train loss 0.1563, val loss 3.8538\n",
      "step 1900: train loss 0.1588, val loss 3.9478\n",
      "step 2000: train loss 0.1488, val loss 3.9286\n",
      "step 2100: train loss 0.1523, val loss 3.9675\n",
      "step 2200: train loss 0.1500, val loss 3.9855\n",
      "step 2300: train loss 0.1429, val loss 3.9846\n",
      "step 2400: train loss 0.1403, val loss 4.0846\n",
      "step 2500: train loss 0.1390, val loss 4.0646\n",
      "step 2600: train loss 0.1339, val loss 4.0581\n",
      "step 2700: train loss 0.1327, val loss 4.1874\n",
      "step 2800: train loss 0.1300, val loss 4.2794\n",
      "step 2900: train loss 0.1354, val loss 4.3300\n",
      "step 3000: train loss 0.1320, val loss 4.3078\n",
      "step 3100: train loss 0.1233, val loss 4.3855\n",
      "step 3200: train loss 0.1292, val loss 4.3297\n",
      "step 3300: train loss 0.1279, val loss 4.2906\n",
      "step 3400: train loss 0.1264, val loss 4.3377\n",
      "step 3500: train loss 0.1300, val loss 4.4434\n",
      "step 3600: train loss 0.1322, val loss 4.4015\n",
      "step 3700: train loss 0.1296, val loss 4.4307\n",
      "step 3800: train loss 0.1193, val loss 4.4064\n",
      "step 3900: train loss 0.1220, val loss 4.3730\n",
      "step 4000: train loss 0.1234, val loss 4.4203\n",
      "step 4100: train loss 0.1227, val loss 4.3643\n",
      "step 4200: train loss 0.1259, val loss 4.4299\n",
      "step 4300: train loss 0.1293, val loss 4.3280\n",
      "step 4400: train loss 0.1155, val loss 4.4746\n",
      "step 4500: train loss 0.1187, val loss 4.6054\n",
      "step 4600: train loss 0.1192, val loss 4.6098\n",
      "step 4700: train loss 0.1155, val loss 4.6142\n",
      "step 4800: train loss 0.1154, val loss 4.4774\n",
      "step 4900: train loss 0.1197, val loss 4.6153\n",
      "step 4999: train loss 0.1154, val loss 4.3543\n"
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters):\n",
    "    # Looping through the training iterations\n",
    "\n",
    "    # Conditional to evaluate model performance periodically\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        # Estimate loss on training and validation datasets\n",
    "        losses = estimate_loss()\n",
    "        # Print the current step and the estimated losses\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # Sampling a batch of data for training\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Forward pass: compute predicted outputs by passing inputs to the model\n",
    "    logits, loss = model(xb, yb)\n",
    "    # Zeroing the gradients of all optimized tensors\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "    # Perform a single optimization step (parameter update)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fd35b9f3-8643-4069-adc5-531ee869ba7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "California.\n",
      "But not only that. Let freedom ring from Stone Mountain of Georgia. Let freedom ring from the mighty\n",
      "mountains of New York. Let freedom ring from the heightening Alleghenies of Pennsylvania. Let\n",
      "freedom ring from the snowcapped Rockies of Coloramath ahersorad the\n",
      "lamen—ons alletenie. We can never be satisfied as long as the Negro is the victim of the unspeakable horrors of\n",
      "police brutality.\n",
      "We can never be satisfied as long as our bodies, heavy with the fatigue of travel, cannot gain lodging in\n",
      "the motels of the highways and the hotels of the cities.\n",
      "We cannot be satisfied as long as the Negro is the victim of the unspeakable horrors of\n",
      "police brutality.\n",
      "We can never be satisfied as long as the Negro in Mississippi cannot vote and the Negro in New York\n",
      "believes he has nothing for which to vote.\n",
      "No, no, we are not satisfied, and we will not be satisfied until justice rolls down like waters and\n",
      "righteousness like a mighty stream. . . .\n",
      "I say to you today, my friends, thze hear that one day on the\n",
      "pursuition and the Negro in New York\n",
      "believes he has nothing for which to vote.\n",
      "No, no, we are not satisfied, and of the pilgrim’s promise thanan. “Mye country, ’tis of\n",
      "thee, sweet land of liberty, of thee I sing. Land where my fathers died, land of the pilgrim’s pride, from\n",
      "every mountain side, let freedom ring.” And if America is to be a great nation, this must become true. So\n",
      "let freedom ring from the prodigious hilltops of New Hampshihese that ablle of ve creesting ghirlls is note freedom and justice.\n",
      "It have a dream that my four little children will one day live in a nation where they will not be judged by\n",
      "the color of their skin butt by the content of their character. I have a dream . . . I have a dream that one\n",
      "day in Alabama, with its vicious racists, with its governor having his lips dripping with the words of\n",
      "interposition and nullificentinor the whords our\n",
      "day in Alabama, with its vicious racists, with its governor having his lips dripping with the word\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22282fda-f90b-43a9-a1ce-c7453eded2a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
